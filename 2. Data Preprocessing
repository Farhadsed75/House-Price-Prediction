import pandas as pd
import numpy as np
from sklearn.cluster import KMeans

data = pd.read_csv("E:/zillow/24/SaleData.csv").drop('Unnamed: 0',axis=1)
data1 = data['RecordingDate'].str.split('-',expand=True).drop(2,axis=1).rename(columns={0:'RecordingDateYear',1:'RecordingDateMonth'})
data = data.join(data1).drop('RecordingDate',axis=1)
data.loc[data['SalesPriceAmount']<10000,'SalesPriceAmount'] = np.nan
numerical_variables = ['RecordingDateYear', 'RecordingDateMonth']
for i in range(len(numerical_variables)):
    if data[numerical_variables[i]].dtypes=='object':
        data[numerical_variables[i]]=data[numerical_variables[i]].astype(float)
data.loc[data['RecordingDateYear']<1975,'RecordingDateYear'] = np.nan
data = data.dropna(subset = ['SalesPriceAmount','RecordingDateYear','RecordingDateMonth'],how='any').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/MarylandUltimateRight.csv").drop(['Unnamed: 0', 'Address'],axis=1)
data = pd.merge(data,data1,how='inner',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/Building.csv").drop('Unnamed: 0',axis=1)
some_values = ['RI000','RI101','RI102','RI103','RI104','RI105',
               'RI106','RI107','RI108','RI109','RI110','RI111',
               'RI112','RI113','RI114','RR000','RR101','RR102',
               'RR103','RR104','RR105','RR106','RR107','RR108',
               'RR109','RR110','RR111','RR112','RR113','RR114',
               'RR115','RR116','RR117','RR118','RR119','RR120','RR999']
data1 = data1.loc[data1['PropertyLandUse'].isin(some_values)].drop_duplicates().sort_values(by='RowID',ignore_index=True)
data = pd.merge(data,data1,how='left',on='RowID').dropna(subset = ['PropertyLandUse'],how='all').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/BuildingAreas.csv").drop('Unnamed: 0',axis=1)
some_values = ['BAG','BAL','BSY','BAT','BAB','BAF']
data1 = data1.loc[data1['BuildingArea'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','BuildingAreaSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('BuildingArea',axis=1).rename(columns={'BuildingAreaSqFt':'LivingBuildingArea'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/BuildingAreas.csv").drop('Unnamed: 0',axis=1)
some_values = ['PRO','PRE','PRC','PRH','PRS']
data1 = data1.loc[data1['BuildingArea'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','BuildingAreaSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('BuildingArea',axis=1).rename(columns={'BuildingAreaSqFt':'Porch'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/Main.csv").drop(['Unnamed: 0','PropertyFullStreetAddress'],axis=1)
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/ExteriorWall.csv").drop('Unnamed: 0',axis=1)
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/ExtraFeature.csv").drop('Unnamed: 0',axis=1)
some_values = ['DCK','DKS','WDD']
data1 = data1.loc[data1['ExtraFeatures'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','ExtraFeaturesSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('ExtraFeatures',axis=1).rename(columns={'ExtraFeaturesSqFt':'Deck'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/Garage.csv").drop('Unnamed: 0',axis=1)
some_values = ['GR','CP','BI','AT','BA']
data1 = data1.loc[data1['Garage'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','GarageAreaSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('Garage',axis=1).rename(columns={'GarageAreaSqFt':'Garage'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/24/TypeConstruction.csv").drop('Unnamed: 0',axis=1)
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

datamd = data
##########################################################################

data = pd.read_csv("E:/zillow/51/SaleData.csv").drop('Unnamed: 0',axis=1)
data1 = data['RecordingDate'].str.split('-',expand=True).drop(2,axis=1).rename(columns={0:'RecordingDateYear',1:'RecordingDateMonth'})
data = data.join(data1).drop('RecordingDate',axis=1)
data.loc[data['SalesPriceAmount']<10000,'SalesPriceAmount'] = np.nan
numerical_variables = ['RecordingDateYear', 'RecordingDateMonth']
for i in range(len(numerical_variables)):
    if data[numerical_variables[i]].dtypes=='object':
        data[numerical_variables[i]]=data[numerical_variables[i]].astype(float)
data.loc[data['RecordingDateYear']<1975,'RecordingDateYear'] = np.nan
data = data.dropna(subset = ['SalesPriceAmount','RecordingDateYear','RecordingDateMonth'],how='any').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/VirginiaUltimateRight.csv").drop(['Unnamed: 0', 'Address'],axis=1)
data = pd.merge(data,data1,how='inner',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/Building.csv").drop('Unnamed: 0',axis=1)
some_values = ['RI000','RI101','RI102','RI103','RI104','RI105',
               'RI106','RI107','RI108','RI109','RI110','RI111',
               'RI112','RI113','RI114','RR000','RR101','RR102',
               'RR103','RR104','RR105','RR106','RR107','RR108',
               'RR109','RR110','RR111','RR112','RR113','RR114',
               'RR115','RR116','RR117','RR118','RR119','RR120','RR999']
data1 = data1.loc[data1['PropertyLandUse'].isin(some_values)].drop_duplicates().sort_values(by='RowID',ignore_index=True)
data = pd.merge(data,data1,how='left',on='RowID').dropna(subset = ['PropertyLandUse'],how='all').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/BuildingAreas.csv").drop('Unnamed: 0',axis=1)
some_values = ['BAG','BAL','BSY','BAT','BAB','BAF']
data1 = data1.loc[data1['BuildingArea'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','BuildingAreaSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('BuildingArea',axis=1).rename(columns={'BuildingAreaSqFt':'LivingBuildingArea'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/BuildingAreas.csv").drop('Unnamed: 0',axis=1)
some_values = ['PRO','PRE','PRC','PRH','PRS']
data1 = data1.loc[data1['BuildingArea'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','BuildingAreaSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('BuildingArea',axis=1).rename(columns={'BuildingAreaSqFt':'Porch'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/Main.csv").drop(['Unnamed: 0','PropertyFullStreetAddress'],axis=1)
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/ExteriorWall.csv").drop('Unnamed: 0',axis=1)
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/ExtraFeature.csv").drop('Unnamed: 0',axis=1)
some_values = ['DCK','DKS','WDD']
data1 = data1.loc[data1['ExtraFeatures'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','ExtraFeaturesSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('ExtraFeatures',axis=1).rename(columns={'ExtraFeaturesSqFt':'Deck'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/Garage.csv").drop('Unnamed: 0',axis=1)
some_values = ['GR','CP','BI','AT','BA']
data1 = data1.loc[data1['Garage'].isin(some_values)]
data1 = data1.sort_values(by=['RowID','GarageAreaSqFt'],ignore_index=True,na_position='first').drop_duplicates(['RowID'],keep='last').drop('Garage',axis=1).rename(columns={'GarageAreaSqFt':'Garage'})
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

data1 = pd.read_csv("E:/zillow/51/TypeConstruction.csv").drop('Unnamed: 0',axis=1)
data = pd.merge(data,data1,how='left',on='RowID').drop_duplicates().sort_values(by='RowID',ignore_index=True)

out = pd.concat([datamd,data],axis=0).drop(['RowID','State'],axis=1).drop_duplicates()
data1 = pd.read_csv("E:/AccessIndex.csv")[['TAZ', 'AM_HWY_Hv3']]
out = pd.merge(out,data1,how='left',on='TAZ').dropna(subset = ['City'],how='all')

out = out.drop(out[(out['RecordingDateYear']==2018)&(out['RecordingDateMonth']==9)].index)
out = out.drop(out[(out['RecordingDateYear']==2018)&(out['RecordingDateMonth']==8)].index)
out = out.drop(out[(out['RecordingDateYear']==2018)&(out['RecordingDateMonth']==7)].index)
out = out.drop_duplicates(subset=out.columns.difference(['RecordingDateYear','RecordingDateMonth'])).sort_values(by='SalesPriceAmount',ignore_index=True)
out = out[['SalesPriceAmount', 'RecordingDateYear', 'RecordingDateMonth', 
           'NoOfStories', 'TotalBathCount', 'LivingBuildingArea', 'LotSizeSquareFeet',
           'Deck', 'Garage', 'Porch', 'PropertyLandUse', 'AM_HWY_Hv3',
           'HeatingType', 'StoryType', 'TypeConstruction', 'ExteriorWall',
           'YearBuilt', 'City', 'County', 'TAZ']]

numerical_variables = ['SalesPriceAmount', 'RecordingDateYear', 'RecordingDateMonth',
                       'NoOfStories', 'TotalBathCount', 'LivingBuildingArea',
                       'LotSizeSquareFeet', 'Deck', 'Garage', 'Porch',
                       'AM_HWY_Hv3','YearBuilt', 'TAZ']
for i in range(len(numerical_variables)):
    if out[numerical_variables[i]].dtypes=='object':
        out[numerical_variables[i]]=out[numerical_variables[i]].astype(float)

out.loc[out['NoOfStories']<1,'NoOfStories'] = np.nan
out.loc[out['TotalBathCount']<=0,'TotalBathCount'] = np.nan
out.loc[out['LivingBuildingArea']<100,'LivingBuildingArea'] = np.nan
out.loc[out['LotSizeSquareFeet']<100,'LotSizeSquareFeet'] = np.nan
out.loc[out['Deck']<10,'Deck'] = np.nan
out.loc[out['Garage']<50,'Garage'] = np.nan
out.loc[out['Porch']<=0,'Porch'] = np.nan
out.loc[out['YearBuilt']<1800,'YearBuilt'] = np.nan
out['PropertyAge'] = out['RecordingDateYear'] - out['YearBuilt']
out.loc[out['PropertyAge']<0,'PropertyAge'] = np.nan
out = out.drop('YearBuilt',axis=1).drop_duplicates().sort_values(by='SalesPriceAmount',ignore_index=True)

num_variables = ['NoOfStories', 'TotalBathCount', 'LivingBuildingArea',
                 'LotSizeSquareFeet', 'Deck', 'Garage', 'Porch','PropertyAge']
for i in range(len(num_variables)):
    out[num_variables[i]] = out.groupby(['TAZ'])[num_variables[i]].transform(lambda x:x.fillna(x.mean()))
out['SalesPriceAmount'] = out['SalesPriceAmount'].round(decimals=0)
out['RecordingDateYear'] = out['RecordingDateYear'].round(decimals=0)
out['RecordingDateMonth'] = out['RecordingDateMonth'].round(decimals=0)
out['NoOfStories'] = ((2*out['NoOfStories']).round(decimals=0))/2
out['TotalBathCount'] = ((2*out['TotalBathCount']).round(decimals=0))/2
out['LivingBuildingArea'] = out['LivingBuildingArea'].round(decimals=0)
out['LotSizeSquareFeet'] = out['LotSizeSquareFeet'].round(decimals=0)
out['Deck'] = out['Deck'].round(decimals=0)
out['Garage'] = out['Garage'].round(decimals=0)
out['Porch'] = out['Porch'].round(decimals=0)
out['AM_HWY_Hv3'] = out['AM_HWY_Hv3'].round(decimals=2)
out['PropertyAge'] = out['PropertyAge'].round(decimals=0)
out['TAZ'] = out['TAZ'].round(decimals=0)
out = out.drop_duplicates().sort_values(by='SalesPriceAmount',ignore_index=True)

out.loc[out['HeatingType']=='ZN','HeatingType'] = 'OT'
out.loc[out['HeatingType']=='CL','HeatingType'] = 'OT'
out.loc[out['HeatingType']=='PR','HeatingType'] = 'OT'
out.loc[out['HeatingType']=='YY','HeatingType'] = 'OT'
out.loc[out['HeatingType']=='SO','HeatingType'] = 'OT'
out.loc[out['HeatingType']=='FL','HeatingType'] = 'OT'
out.loc[out['ExteriorWall']=='CB','ExteriorWall'] = 'OT'
out.loc[out['ExteriorWall']=='SH','ExteriorWall'] = 'OT'
out.loc[out['ExteriorWall']=='CC','ExteriorWall'] = 'OT'
out.loc[out['ExteriorWall']=='GL','ExteriorWall'] = 'OT'
out.loc[out['ExteriorWall']=='TU','ExteriorWall'] = 'OT'
out.loc[out['ExteriorWall']=='AD','ExteriorWall'] = 'OT'
out.loc[out['ExteriorWall']=='BV','ExteriorWall'] = 'BR'
out.loc[out['ExteriorWall']=='LG','ExteriorWall'] = 'WD'
out.loc[out['TypeConstruction']=='WD','TypeConstruction'] = 'LG'
out.loc[out['PropertyLandUse']=='RI104','PropertyLandUse'] = 'RI112'
out.loc[out['PropertyLandUse']=='RI107','PropertyLandUse'] = 'RI112'
out.loc[out['PropertyLandUse']=='RR118','PropertyLandUse'] = 'RR101'

out['HeatingType'] = out.groupby(['TAZ'])['HeatingType'].transform(lambda x:x.fillna(x.value_counts().index.tolist()[0] if len(x.value_counts().index.tolist())!=0 else np.nan))
out['StoryType'] = out.groupby(['TAZ'])['StoryType'].transform(lambda x:x.fillna(x.value_counts().index.tolist()[0] if len(x.value_counts().index.tolist())!=0 else np.nan))
out['TypeConstruction'] = out.groupby(['TAZ'])['TypeConstruction'].transform(lambda x:x.fillna(x.value_counts().index.tolist()[0] if len(x.value_counts().index.tolist())!=0 else np.nan))
out['ExteriorWall'] = out.groupby(['TAZ'])['ExteriorWall'].transform(lambda x:x.fillna(x.value_counts().index.tolist()[0] if len(x.value_counts().index.tolist())!=0 else np.nan))
out = out.dropna(subset = ['HeatingType','StoryType','TypeConstruction','ExteriorWall'],how='any').drop_duplicates().sort_values(by='SalesPriceAmount',ignore_index=True)

def outlier(variable):
    global out
    IQR=out[variable].quantile(0.75)-out[variable].quantile(0.25)
    UE=out[variable].quantile(0.75)+1.5*IQR
    maximum=out[variable].max()

    while UE<maximum :
        out.loc[out[variable]>UE,variable]=np.nan 
        out=out.drop_duplicates()
        IQR=out[variable].quantile(0.75)-out[variable].quantile(0.25)
        UE=out[variable].quantile(0.75)+1.5*IQR
        maximum=out[variable].max()

    IQR=out[variable].quantile(0.75)-out[variable].quantile(0.25)
    LE=out[variable].quantile(0.25)-1.5*IQR
    minimum=out[variable].min()

    while LE>minimum :
        out.loc[out[variable]<LE,variable]=np.nan 
        out=out.drop_duplicates()
        IQR=out[variable].quantile(0.75)-out[variable].quantile(0.25)
        LE=out[variable].quantile(0.25)-1.5*IQR
        minimum=out[variable].min()
    return out

def chechoutlier(variable,counter):
    global out
    IQR = out[variable].quantile(0.75)-out[variable].quantile(0.25)
    UE = out[variable].quantile(0.75)+1.5*IQR
    LE = out[variable].quantile(0.25)-1.5*IQR
    maximum = out[variable].max()
    minimum = out[variable].min()
    if UE<maximum or LE>minimum:
        counter = 'NG'
        out = outlier(variable)  
        out = out.dropna(subset = [variable],how='all')
    else:
        counter = 'OK'
    return counter

def chechoutlier1(variable,counter):
    global out
    while counter == 'NG':
        IQR = out[variable].quantile(0.75)-out[variable].quantile(0.25)
        UE = out[variable].quantile(0.75)+1.5*IQR
        LE = out[variable].quantile(0.25)-1.5*IQR
        maximum = out[variable].max()
        minimum = out[variable].min()
        if UE<maximum or LE>minimum:
            counter = 'NG'
            out = outlier(variable)  
            out[variable] = out.groupby(['TAZ'])[variable].transform(lambda x:x.fillna(x.mean()))
            out[variable] = out[variable].round(decimals=0)
        else:
            counter = 'OK'
    return counter

def chechoutlier2(variable,counter):
    global out
    while counter == 'NG':
        IQR = out[variable].quantile(0.75)-out[variable].quantile(0.25)
        UE = out[variable].quantile(0.75)+1.5*IQR
        LE = out[variable].quantile(0.25)-1.5*IQR
        maximum = out[variable].max()
        minimum = out[variable].min()
        if UE<maximum or LE>minimum:
            counter = 'NG'
            out = outlier(variable)  
            out[variable] = out.groupby(['TAZ'])[variable].transform(lambda x:x.fillna(x.mean()))
            out[variable] = ((2*out[variable]).round(decimals=0))/2
        else:
            counter = 'OK'
    return counter

output = 'NG'

while output == 'NG':
    dict={}
    for i in range(10):
        dict['outlier'+str(i)]="NG"
    locals().update(dict)
    
    while outlier0 == 'NG' or outlier1 == 'NG':
        outlier0 = chechoutlier('SalesPriceAmount',outlier0)
        outlier1 = chechoutlier('AM_HWY_Hv3',outlier1)
    
    outlier2 = chechoutlier2('NoOfStories',outlier2)
    outlier3 = chechoutlier2('TotalBathCount',outlier3)
    outlier4 = chechoutlier1('LivingBuildingArea',outlier4)
    outlier5 = chechoutlier1('LotSizeSquareFeet',outlier5)
    outlier6 = chechoutlier1('Deck',outlier6)
    outlier7 = chechoutlier1('Garage',outlier7)
    outlier8 = chechoutlier1('Porch',outlier8)
    outlier9 = chechoutlier1('PropertyAge',outlier9)
    if out.isna().sum().sum()!=0:
        out = out.dropna(subset = ['NoOfStories','TotalBathCount','LivingBuildingArea',
                                   'LotSizeSquareFeet','Deck','Garage','Porch','PropertyAge'],how='any').drop_duplicates().sort_values(by='SalesPriceAmount',ignore_index=True)
    else:
        output = 'OK'

datamd_array = out[['SalesPriceAmount']].values.astype(float)
kmn = KMeans(n_clusters=4)
kmn.fit(datamd_array)
labels = kmn.predict(datamd_array)
out['cluster'] = labels
out['cluster1'] = out.groupby(['City'])['cluster'].transform(lambda x: x.value_counts().index.tolist()[0] if len(x.value_counts().index.tolist())!=0 else np.nan)
b = {}
for i in range(4):
    b[i] = out[out['cluster1']==i]['SalesPriceAmount'].mean()
bb = sorted(b.items(), key=lambda x:x[1])
out['cluster1'] = out['cluster1'].replace(bb[0][0],'LowPrice_City')
out['cluster1'] = out['cluster1'].replace(bb[1][0],'AveragePrice_City')
out['cluster1'] = out['cluster1'].replace(bb[2][0],'HighPrice_City')
out['cluster1'] = out['cluster1'].replace(bb[3][0],'VeryHighPrice_City')
out = out.drop(['cluster', 'City'],axis=1)
out = out.drop_duplicates().sort_values(by='SalesPriceAmount',ignore_index=True)

output = 'NG'

while output == 'NG':
    dict={}
    for i in range(10):
        dict['outlier'+str(i)]="NG"
    locals().update(dict)
    
    while outlier0 == 'NG' or outlier1 == 'NG':
        outlier0 = chechoutlier('SalesPriceAmount',outlier0)
        outlier1 = chechoutlier('AM_HWY_Hv3',outlier1)
    
    outlier2 = chechoutlier2('NoOfStories',outlier2)
    outlier3 = chechoutlier2('TotalBathCount',outlier3)
    outlier4 = chechoutlier1('LivingBuildingArea',outlier4)
    outlier5 = chechoutlier1('LotSizeSquareFeet',outlier5)
    outlier6 = chechoutlier1('Deck',outlier6)
    outlier7 = chechoutlier1('Garage',outlier7)
    outlier8 = chechoutlier1('Porch',outlier8)
    outlier9 = chechoutlier1('PropertyAge',outlier9)
    if out.isna().sum().sum()!=0:
        out = out.dropna(subset = ['NoOfStories','TotalBathCount','LivingBuildingArea',
                                   'LotSizeSquareFeet','Deck','Garage','Porch','PropertyAge'],how='any').drop_duplicates().sort_values(by='SalesPriceAmount',ignore_index=True)
    else:
        output = 'OK'

out1 = out[['SalesPriceAmount','RecordingDateYear','RecordingDateMonth']]
out1['SalesPriceAmount'] = out1.groupby(['RecordingDateYear','RecordingDateMonth'])['SalesPriceAmount'].transform(lambda x:x.mean())
out1['SalesPriceAmount'] = out1['SalesPriceAmount'].round(decimals=0)
out1 = out1.drop_duplicates().sort_values(by=['RecordingDateYear','RecordingDateMonth'],ignore_index=True).rename(columns={'SalesPriceAmount':'AvgSalesPriceAmount'})

out = pd.merge(out,out1,how='left',on=['RecordingDateYear','RecordingDateMonth'])
out['SalesPriceAmount'] = out['SalesPriceAmount'] - out['AvgSalesPriceAmount']
out = out.drop(['RecordingDateYear','RecordingDateMonth'] ,axis=1)
out = out.drop_duplicates(subset=out.columns.difference(['AvgSalesPriceAmount']))

output = 'NG'

while output == 'NG':
    dict={}
    for i in range(10):
        dict['outlier'+str(i)]="NG"
    locals().update(dict)
    
    while outlier0 == 'NG' or outlier1 == 'NG':
        outlier0 = chechoutlier('SalesPriceAmount',outlier0)
        outlier1 = chechoutlier('AM_HWY_Hv3',outlier1)
    
    outlier2 = chechoutlier2('NoOfStories',outlier2)
    outlier3 = chechoutlier2('TotalBathCount',outlier3)
    outlier4 = chechoutlier1('LivingBuildingArea',outlier4)
    outlier5 = chechoutlier1('LotSizeSquareFeet',outlier5)
    outlier6 = chechoutlier1('Deck',outlier6)
    outlier7 = chechoutlier1('Garage',outlier7)
    outlier8 = chechoutlier1('Porch',outlier8)
    outlier9 = chechoutlier1('PropertyAge',outlier9)
    if out.isna().sum().sum()!=0:
        out = out.dropna(subset = ['NoOfStories','TotalBathCount','LivingBuildingArea',
                                   'LotSizeSquareFeet','Deck','Garage','Porch','PropertyAge'],how='any').drop_duplicates().sort_values(by='SalesPriceAmount',ignore_index=True)
    else:
        output = 'OK'

out = out.rename(columns={'PropertyLandUse':''})
out = pd.get_dummies(out,columns=[''],prefix='LandUse',prefix_sep='_')
out = out.rename(columns={'HeatingType':''})
out = pd.get_dummies(out,columns=[''],prefix='Heating',prefix_sep='_')
out = out.rename(columns={'StoryType':''})
out = pd.get_dummies(out,columns=[''],prefix='StoryType',prefix_sep='_')
out = out.rename(columns={'TypeConstruction':''})
out = pd.get_dummies(out,columns=[''],prefix='TypeConstruction',prefix_sep='_')
out = out.rename(columns={'ExteriorWall':''})
out = pd.get_dummies(out,columns=[''],prefix='ExWall',prefix_sep='_')
out = out.rename(columns={'County':''})
out = pd.get_dummies(out,columns=[''],prefix='',prefix_sep='')
out = out.rename(columns={'cluster1':''})
out = pd.get_dummies(out,columns=[''],prefix='',prefix_sep='')

out = out.drop(['Heating_HW','StoryType_SFA','TypeConstruction_SL',
                'ExWall_BL','LOUDOUN','VeryHighPrice_City','LandUse_RI111','TAZ'],axis=1)
out = out.drop_duplicates(subset=out.columns.difference(['AvgSalesPriceAmount'])).sort_values(by='SalesPriceAmount',ignore_index=True)
out = out[['SalesPriceAmount', 'NoOfStories', 'TotalBathCount',
           'LivingBuildingArea', 'LotSizeSquareFeet', 'Deck', 'Garage', 'Porch',
           'LandUse_RI000', 'LandUse_RI101', 'LandUse_RI106', 'LandUse_RI109', 
           'LandUse_RI110', 'LandUse_RI112', 'LandUse_RI113', 'LandUse_RR000', 
           'LandUse_RR101', 'LandUse_RR999', 'LandUse_RR102', 'LandUse_RR103', 
           'LandUse_RR104', 'LandUse_RR106', 'LandUse_RR108', 'LandUse_RR109', 
           'LandUse_RR110', 'LandUse_RR115', 'LandUse_RR117', 'AM_HWY_Hv3', 
           'Heating_BB', 'Heating_CE', 'Heating_EL', 'Heating_FA', 'Heating_GS', 
           'Heating_HP', 'Heating_NO', 'Heating_OL', 'Heating_OT', 'Heating_RD', 
           'Heating_SM', 'Heating_SS', 'StoryType_ANB', 'StoryType_ATC', 
           'StoryType_BIL', 'StoryType_BLB', 'StoryType_BMT', 'StoryType_LVL', 
           'StoryType_SEB', 'StoryType_SFB', 'StoryType_SLA', 'StoryType_SLB', 
           'StoryType_SPE', 'StoryType_SPF', 'StoryType_SPL', 
           'TypeConstruction_BR', 'TypeConstruction_CB', 'TypeConstruction_CO', 
           'TypeConstruction_FR', 'TypeConstruction_LG', 'TypeConstruction_MY', 
           'TypeConstruction_OT', 'TypeConstruction_ST', 'ExWall_SD', 'ExWall_OT', 
           'ExWall_BR', 'ExWall_AH', 'ExWall_ST', 'ExWall_RK', 'ExWall_WD', 
           'ExWall_WG', 'ExWall_MT', 'ExWall_CM', 'ExWall_MY', 'ExWall_WS', 
           'PropertyAge', 'AveragePrice_City', 'HighPrice_City', 'LowPrice_City',
           'ALEXANDRIA CITY', 'ANNE ARUNDEL', 'CALVERT', 'CARROLL', 'CHARLES', 
           'FAIRFAX', 'FREDERICK', 'HOWARD', 'MONTGOMERY', 'PRINCE GEORGES', 
           'ST MARYS', 'STAFFORD', 'AvgSalesPriceAmount']]

namelist = ['InterestRate_BankPrimeLoanRate','Homeownership Rate(USA)','Median Number of Months on Sales Market(USA)X',
            'Monthly Supply of Houses(USA)','New Houses for Sale(USA)X','NewPrivateHousingAuthorized(USA)','Unemployment Rate(USA)']
for i in namelist:
    data = pd.read_csv('E:/%s.csv'%(i)).drop('DATE',axis=1)
    out1 = pd.merge(out1,data,how='left',on=['RecordingDateYear','RecordingDateMonth'])
out1 = out1.drop_duplicates()

out.to_csv("E:/dataset.csv")
out1.to_csv("E:/dataset1.csv")

df0 = out1[['SalesPriceAmount', 'RecordingDateYear', 'RecordingDateMonth']]
df = pd.read_csv('E:/PayanNameh Desktop/Economic Features/Final/Final/InterestRate_BankPrimeLoanRate.csv')
df = df.drop(df[(df['RecordingDateYear']>2020)|(df['RecordingDateYear']<1975)].index)
df = pd.merge(df,df0,how='left',on=['RecordingDateYear','RecordingDateMonth'])
df = df[['DATE', 'SalesPriceAmount', 'RecordingDateYear', 'RecordingDateMonth', 'InterestRate']]

namelist = ['Homeownership Rate(USA)','Median Number of Months on Sales Market(USA)X',
            'Monthly Supply of Houses(USA)','New Houses for Sale(USA)X','NewPrivateHousingAuthorized(USA)','Unemployment Rate(USA)']
for i in namelist:
    data = pd.read_csv('E:/PayanNameh Desktop/Economic Features/Final/Final/%s.csv'%(i)).drop('DATE',axis=1)
    df = pd.merge(df,data,how='left',on=['RecordingDateYear','RecordingDateMonth'])

df = df[['SalesPriceAmount', 'RecordingDateYear', 'RecordingDateMonth',
         'InterestRate', 'HomeownershipRate', 'MonthsOnSales', 'SupplyOfHouses',
         'NewHousesForSale', 'NewHousesAuthorized', 'UnemploymentRate', 'DATE']]
dict={}
for i in range(8):
    dict['x'+str(i)]=df.columns[2+i]
    df['x'+str(i)] = np.nan
for i in range(1,8):
    df[list(dict)[i]] = 2*(df[dict[list(dict)[i]]]-df[12:522][dict[list(dict)[i]]].min())/(df[12:522][dict[list(dict)[i]]].max()-df[12:522][dict[list(dict)[i]]].min())-1
    df[list(dict)[i]][0] = df[12:522][dict[list(dict)[i]]].min()
    df[list(dict)[i]][1] = df[12:522][dict[list(dict)[i]]].max()

df.iloc[2:12,12:] = np.nan
df.iloc[522:,0] = 1
df.to_csv("E:/dataset1_Predict.csv")

df1 = df.copy()
df1.iloc[492:,0] = 1
df1.to_csv("E:/dataset1_Predict1.csv")
